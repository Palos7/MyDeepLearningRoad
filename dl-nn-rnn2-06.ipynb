{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/tutorials/text/text_generation\n\n# 循环神经网络（RNN）文本生成\n\n本教程演示如何使用基于字符的 RNN 生成文本。我们将使用 Andrej Karpathy 在《循环神经网络不合理的有效性》一文中提供的莎士比亚作品数据集。给定此数据中的一个字符序列 （“Shakespear”），训练一个模型以预测该序列的下一个字符（“e”）。通过重复调用该模型，可以生成更长的文本序列。","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:48.239788Z","iopub.execute_input":"2021-05-29T11:25:48.240387Z","iopub.status.idle":"2021-05-29T11:25:54.796327Z","shell.execute_reply.started":"2021-05-29T11:25:48.240284Z","shell.execute_reply":"2021-05-29T11:25:54.795158Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n                'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:54.799274Z","iopub.execute_input":"2021-05-29T11:25:54.799992Z","iopub.status.idle":"2021-05-29T11:25:55.307195Z","shell.execute_reply.started":"2021-05-29T11:25:54.799937Z","shell.execute_reply":"2021-05-29T11:25:55.305867Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n1122304/1115394 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# 读取并为 py2 compat 解码\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n\n# 文本长度是指文本中的字符个数\nprint ('Length of text: {} characters'.format(len(text)))\n# 看一看文本中的前 250 个字符\nprint(text[:250])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.309849Z","iopub.execute_input":"2021-05-29T11:25:55.310389Z","iopub.status.idle":"2021-05-29T11:25:55.321470Z","shell.execute_reply.started":"2021-05-29T11:25:55.310342Z","shell.execute_reply":"2021-05-29T11:25:55.319958Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Length of text: 1115394 characters\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# 文本中的非重复字符\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.323920Z","iopub.execute_input":"2021-05-29T11:25:55.324448Z","iopub.status.idle":"2021-05-29T11:25:55.348248Z","shell.execute_reply.started":"2021-05-29T11:25:55.324405Z","shell.execute_reply":"2021-05-29T11:25:55.346559Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"65 unique characters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 处理文本\n### 向量化文本\n在训练之前，我们需要将字符串映射到数字表示值。创建两个查找表格：一个将字符映射到数字，另一个将数字映射到字符。","metadata":{}},{"cell_type":"code","source":"# 创建从非重复字符到索引的映射\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.350404Z","iopub.execute_input":"2021-05-29T11:25:55.350939Z","iopub.status.idle":"2021-05-29T11:25:55.619480Z","shell.execute_reply.started":"2021-05-29T11:25:55.350892Z","shell.execute_reply":"2021-05-29T11:25:55.618306Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#现在，每个字符都有一个整数表示值。请注意，我们将字符映射至索引 0 至 len(unique).\nprint('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.620960Z","iopub.execute_input":"2021-05-29T11:25:55.621405Z","iopub.status.idle":"2021-05-29T11:25:55.633647Z","shell.execute_reply.started":"2021-05-29T11:25:55.621362Z","shell.execute_reply":"2021-05-29T11:25:55.632112Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  '$' :   3,\n  '&' :   4,\n  \"'\" :   5,\n  ',' :   6,\n  '-' :   7,\n  '.' :   8,\n  '3' :   9,\n  ':' :  10,\n  ';' :  11,\n  '?' :  12,\n  'A' :  13,\n  'B' :  14,\n  'C' :  15,\n  'D' :  16,\n  'E' :  17,\n  'F' :  18,\n  'G' :  19,\n  ...\n}\n","output_type":"stream"}]},{"cell_type":"code","source":"# 显示文本首 13 个字符的整数映射\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.635605Z","iopub.execute_input":"2021-05-29T11:25:55.635955Z","iopub.status.idle":"2021-05-29T11:25:55.649888Z","shell.execute_reply.started":"2021-05-29T11:25:55.635927Z","shell.execute_reply":"2021-05-29T11:25:55.648393Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 预测任务\n给定一个字符或者一个字符序列，下一个最可能出现的字符是什么？这就是我们训练模型要执行的任务。输入进模型的是一个字符序列，我们训练这个模型来预测输出 -- 每个时间步（time step）预测下一个字符是什么。\n\n由于 RNN 是根据前面看到的元素维持内部状态，那么，给定此时计算出的所有字符，下一个字符是什么？\n### 创建训练样本和目标\n接下来，将文本划分为样本序列。每个输入序列包含文本中的 seq_length 个字符。\n\n对于每个输入序列，其对应的目标包含相同长度的文本，但是向右顺移一个字符。\n\n将文本拆分为长度为 seq_length+1 的文本块。例如，假设 seq_length 为 4 而且文本为 “Hello”， 那么输入序列将为 “Hell”，目标序列将为 “ello”。\n\n为此，首先使用 tf.data.Dataset.from_tensor_slices 函数把文本向量转换为字符索引流。","metadata":{}},{"cell_type":"code","source":"# 设定每个输入句子长度的最大值\nseq_length = 100\nexamples_per_epoch = len(text)//seq_length\n\n# 创建训练样本 / 目标\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:55.654698Z","iopub.execute_input":"2021-05-29T11:25:55.655006Z","iopub.status.idle":"2021-05-29T11:25:57.902671Z","shell.execute_reply.started":"2021-05-29T11:25:55.654973Z","shell.execute_reply":"2021-05-29T11:25:57.901539Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"F\ni\nr\ns\nt\n","output_type":"stream"}]},{"cell_type":"code","source":"#batch 方法使我们能轻松把单个字符转换为所需长度的序列。\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:57.905028Z","iopub.execute_input":"2021-05-29T11:25:57.905463Z","iopub.status.idle":"2021-05-29T11:25:57.929806Z","shell.execute_reply.started":"2021-05-29T11:25:57.905422Z","shell.execute_reply":"2021-05-29T11:25:57.928270Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","output_type":"stream"}]},{"cell_type":"code","source":"#对于每个序列，使用 map 方法先复制再顺移，以创建输入文本和目标文本。map 方法可以将一个简单的函数应用到每一个批次 （batch）。\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:57.931434Z","iopub.execute_input":"2021-05-29T11:25:57.931853Z","iopub.status.idle":"2021-05-29T11:25:58.008297Z","shell.execute_reply.started":"2021-05-29T11:25:57.931813Z","shell.execute_reply":"2021-05-29T11:25:58.007220Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#打印第一批样本的输入与目标值：\nfor input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.009785Z","iopub.execute_input":"2021-05-29T11:25:58.010346Z","iopub.status.idle":"2021-05-29T11:25:58.079822Z","shell.execute_reply.started":"2021-05-29T11:25:58.010302Z","shell.execute_reply":"2021-05-29T11:25:58.078548Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\nTarget data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"这些向量的每个索引均作为一个时间步来处理。作为时间步 0 的输入，模型接收到 “F” 的索引，并尝试预测 “i” 的索引为下一个字符。在下一个时间步，模型执行相同的操作，但是 RNN 不仅考虑当前的输入字符，还会考虑上一步的信息。","metadata":{}},{"cell_type":"code","source":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.081511Z","iopub.execute_input":"2021-05-29T11:25:58.082218Z","iopub.status.idle":"2021-05-29T11:25:58.411978Z","shell.execute_reply.started":"2021-05-29T11:25:58.082169Z","shell.execute_reply":"2021-05-29T11:25:58.410767Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Step    0\n  input: 18 ('F')\n  expected output: 47 ('i')\nStep    1\n  input: 47 ('i')\n  expected output: 56 ('r')\nStep    2\n  input: 56 ('r')\n  expected output: 57 ('s')\nStep    3\n  input: 57 ('s')\n  expected output: 58 ('t')\nStep    4\n  input: 58 ('t')\n  expected output: 1 (' ')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 创建训练批次\n前面我们使用 tf.data 将文本拆分为可管理的序列。但是在把这些数据输送至模型之前，我们需要将数据重新排列 （shuffle） 并打包为批次。","metadata":{}},{"cell_type":"code","source":"# 批大小\nBATCH_SIZE = 64\n\n# 设定缓冲区大小，以重新排列数据集\n# （TF 数据被设计为可以处理可能是无限的序列，\n# 所以它不会试图在内存中重新排列整个序列。相反，\n# 它维持一个缓冲区，在缓冲区重新排列元素。） \nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.414604Z","iopub.execute_input":"2021-05-29T11:25:58.414997Z","iopub.status.idle":"2021-05-29T11:25:58.433154Z","shell.execute_reply.started":"2021-05-29T11:25:58.414933Z","shell.execute_reply":"2021-05-29T11:25:58.431919Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 创建模型\n使用 tf.keras.Sequential 定义模型。在这个简单的例子中，我们使用了三个层来定义模型：\n\n* tf.keras.layers.Embedding：输入层。一个可训练的对照表，它会将每个字符的数字映射到一个 embedding_dim 维度的向量。\n* tf.keras.layers.GRU：一种 RNN 类型，其大小由 units=rnn_units 指定（这里你也可以使用一个 LSTM 层）。\n* tf.keras.layers.Dense：输出层，带有 vocab_size 个输出。","metadata":{}},{"cell_type":"code","source":"# 词集的长度\nvocab_size = len(vocab)\n\n# 嵌入的维度\nembedding_dim = 256\n\n# RNN 的单元数量\nrnn_units = 1024","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.434618Z","iopub.execute_input":"2021-05-29T11:25:58.435124Z","iopub.status.idle":"2021-05-29T11:25:58.440672Z","shell.execute_reply.started":"2021-05-29T11:25:58.435027Z","shell.execute_reply":"2021-05-29T11:25:58.439325Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.442912Z","iopub.execute_input":"2021-05-29T11:25:58.443934Z","iopub.status.idle":"2021-05-29T11:25:58.453300Z","shell.execute_reply.started":"2021-05-29T11:25:58.443885Z","shell.execute_reply":"2021-05-29T11:25:58.451977Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.455031Z","iopub.execute_input":"2021-05-29T11:25:58.455833Z","iopub.status.idle":"2021-05-29T11:25:58.766904Z","shell.execute_reply.started":"2021-05-29T11:25:58.455580Z","shell.execute_reply":"2021-05-29T11:25:58.765783Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           16640     \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:25:58.769570Z","iopub.execute_input":"2021-05-29T11:25:58.770012Z","iopub.status.idle":"2021-05-29T11:26:03.067852Z","shell.execute_reply.started":"2021-05-29T11:25:58.769931Z","shell.execute_reply":"2021-05-29T11:26:03.066748Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"为了获得模型的实际预测，我们需要从输出分布中抽样，以获得实际的字符索引。这个分布是根据对字符集的逻辑回归定义的。\n\n请注意：从这个分布中 抽样 很重要，因为取分布的 最大值自变量点集（argmax） 很容易使模型卡在循环中。\n\n试试这个批次中的第一个样本：","metadata":{}},{"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n#这使我们得到每个时间步预测的下一个字符的索引。\nsampled_indices","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:06.210401Z","iopub.execute_input":"2021-05-29T11:27:06.210847Z","iopub.status.idle":"2021-05-29T11:27:06.226191Z","shell.execute_reply.started":"2021-05-29T11:27:06.210814Z","shell.execute_reply":"2021-05-29T11:27:06.224846Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([ 6, 63, 43, 47, 22, 16, 48, 56, 31, 17, 17, 61, 30, 35, 35, 35, 19,\n       31,  4, 46, 31,  7, 64, 47, 50, 33, 40, 46, 35, 60, 37, 15, 27, 53,\n       61, 30, 30, 54, 32, 11, 60, 36, 55, 41, 22, 41, 13, 19, 38, 39, 29,\n       22, 39, 30, 45, 15, 18,  9, 49, 22, 38, 24, 37, 63, 45, 33,  6,  5,\n       43,  7, 57, 45, 60,  2, 20, 22, 39, 23, 39, 25, 49,  8, 45, 14, 57,\n       32, 47, 51, 38, 14,  0,  6, 13, 16, 22, 32,  8, 61, 43, 39])"},"metadata":{}}]},{"cell_type":"code","source":"#解码它们，以查看此未经训练的模型预测的文本：\nprint(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:11.566072Z","iopub.execute_input":"2021-05-29T11:27:11.566498Z","iopub.status.idle":"2021-05-29T11:27:11.577476Z","shell.execute_reply.started":"2021-05-29T11:27:11.566464Z","shell.execute_reply":"2021-05-29T11:27:11.575810Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Input: \n 'on,\\nWho is already sick and pale with grief,\\nThat thou her maid art far more fair than she:\\nBe not h'\n\nNext Char Predictions: \n \",yeiJDjrSEEwRWWWGS&hS-zilUbhWvYCOowRRpT;vXqcJcAGZaQJaRgCF3kJZLYygU,'e-sgv!HJaKaMk.gBsTimZB\\n,ADJT.wea\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 训练模型\n此时，这个问题可以被视为一个标准的分类问题：给定先前的 RNN 状态和这一时间步的输入，预测下一个字符的类别。\n\n### 添加优化器和损失函数\n标准的 tf.keras.losses.sparse_categorical_crossentropy 损失函数在这里适用，因为它被应用于预测的最后一个维度。\n\n因为我们的模型返回逻辑回归，所以我们需要设定命令行参数 from_logits。","metadata":{}},{"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:20.350009Z","iopub.execute_input":"2021-05-29T11:27:20.350389Z","iopub.status.idle":"2021-05-29T11:27:20.370507Z","shell.execute_reply.started":"2021-05-29T11:27:20.350358Z","shell.execute_reply":"2021-05-29T11:27:20.369157Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       4.1759\n","output_type":"stream"}]},{"cell_type":"code","source":"#使用 tf.keras.Model.compile 方法配置训练步骤。我们将使用 tf.keras.optimizers.Adam 并采用默认参数，以及损失函数。\nmodel.compile(optimizer='adam', loss=loss)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:23.546406Z","iopub.execute_input":"2021-05-29T11:27:23.546839Z","iopub.status.idle":"2021-05-29T11:27:23.566549Z","shell.execute_reply.started":"2021-05-29T11:27:23.546809Z","shell.execute_reply":"2021-05-29T11:27:23.565160Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## 配置检查点\n使用 tf.keras.callbacks.ModelCheckpoint 来确保训练过程中保存检查点。","metadata":{}},{"cell_type":"code","source":"# 检查点保存至的目录\ncheckpoint_dir = './training_checkpoints'\n\n# 检查点的文件名\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:27.317127Z","iopub.execute_input":"2021-05-29T11:27:27.317513Z","iopub.status.idle":"2021-05-29T11:27:27.323990Z","shell.execute_reply.started":"2021-05-29T11:27:27.317479Z","shell.execute_reply":"2021-05-29T11:27:27.321991Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## 执行训练\n为保持训练时间合理，使用 10 个周期来训练模型。在 Colab 中，将运行时设置为 GPU 以加速训练。","metadata":{}},{"cell_type":"code","source":"EPOCHS=10\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:27:30.535508Z","iopub.execute_input":"2021-05-29T11:27:30.535966Z","iopub.status.idle":"2021-05-29T11:29:10.661381Z","shell.execute_reply.started":"2021-05-29T11:27:30.535921Z","shell.execute_reply":"2021-05-29T11:29:10.660301Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/10\n172/172 [==============================] - 12s 43ms/step - loss: 3.2001\nEpoch 2/10\n172/172 [==============================] - 10s 43ms/step - loss: 2.0453\nEpoch 3/10\n172/172 [==============================] - 10s 44ms/step - loss: 1.7363\nEpoch 4/10\n172/172 [==============================] - 10s 43ms/step - loss: 1.5629\nEpoch 5/10\n172/172 [==============================] - 10s 43ms/step - loss: 1.4687\nEpoch 6/10\n172/172 [==============================] - 10s 44ms/step - loss: 1.4005\nEpoch 7/10\n172/172 [==============================] - 10s 43ms/step - loss: 1.3090\nEpoch 9/10\n172/172 [==============================] - 10s 43ms/step - loss: 1.2716\nEpoch 10/10\n172/172 [==============================] - 10s 43ms/step - loss: 1.2340\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 生成文本\n### 恢复最新的检查点\n为保持此次预测步骤简单，将批大小设定为 1。\n\n由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n\n若要使用不同的 batch_size 来运行模型，我们需要重建模型并从检查点中恢复权重。","metadata":{}},{"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:29:36.985400Z","iopub.execute_input":"2021-05-29T11:29:36.985760Z","iopub.status.idle":"2021-05-29T11:29:36.997908Z","shell.execute_reply.started":"2021-05-29T11:29:36.985731Z","shell.execute_reply":"2021-05-29T11:29:36.996429Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'./training_checkpoints/ckpt_10'"},"metadata":{}}]},{"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:29:40.903921Z","iopub.execute_input":"2021-05-29T11:29:40.904370Z","iopub.status.idle":"2021-05-29T11:29:41.191645Z","shell.execute_reply.started":"2021-05-29T11:29:40.904310Z","shell.execute_reply":"2021-05-29T11:29:41.189428Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\ngru_1 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 4,021,569\nTrainable params: 4,021,569\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 预测循环\n### 下面的代码块生成文本：\n\n* 首先设置起始字符串，初始化 RNN 状态并设置要生成的字符个数。\n\n* 用起始字符串和 RNN 状态，获取下一个字符的预测分布。\n\n* 然后，用分类分布计算预测字符的索引。把这个预测字符当作模型的下一个输入。\n\n* 模型返回的 RNN 状态被输送回模型。现在，模型有更多上下文可以学习，而非只有一个字符。在预测出下一个字符后，更改过的 RNN 状态被再次输送回模型。模型就是这样，通过不断从前面预测的字符获得更多上下文，进行学习。","metadata":{}},{"cell_type":"code","source":"def generate_text(model, start_string):\n  # 评估步骤（用学习过的模型生成文本）\n\n  # 要生成的字符个数\n  num_generate = 1000\n\n  # 将起始字符串转换为数字（向量化）\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # 空字符串用于存储结果\n  text_generated = []\n\n  # 低温度会生成更可预测的文本\n  # 较高温度会生成更令人惊讶的文本\n  # 可以通过试验以找到最好的设定\n  temperature = 1.0\n\n  # 这里批大小为 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # 删除批次的维度\n      predictions = tf.squeeze(predictions, 0)\n\n      # 用分类分布预测模型返回的字符\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:29:45.813588Z","iopub.execute_input":"2021-05-29T11:29:45.813938Z","iopub.status.idle":"2021-05-29T11:29:45.822688Z","shell.execute_reply.started":"2021-05-29T11:29:45.813907Z","shell.execute_reply":"2021-05-29T11:29:45.821446Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(generate_text(model, start_string=u\"ROMEO: \"))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:29:48.009267Z","iopub.execute_input":"2021-05-29T11:29:48.009621Z","iopub.status.idle":"2021-05-29T11:29:54.003559Z","shell.execute_reply.started":"2021-05-29T11:29:48.009593Z","shell.execute_reply":"2021-05-29T11:29:54.002540Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"ROMEO: there is child,\nDo me pows be fulfion daughter.\nWhat is the world: I hee no more;'\nPrith either your warse;\nNot of your hotour friend, and to thy way in\nthis prince from off the garden\nIn quiet that roin on sharp to my son purch.\nYou shall have the city have sworn out of you and\nlearness, to dust thou not to be\nDowble to her eriand, blended, as it stamen:\nAs it were in slie lord; and they ster up,\nProcure of the power may push-plothets Edward's crutcher? be yone.\n\nLUCit a\nI will my friend nor, if I hither come,\nAnd Petruchio is least.\n\nFirst Senator:\nThey, 'tis enamious with you, a thieves,\nWhen you are profit-for son.\n\nYORK:\nO, let them go; the singly demess reclieve\nI give my gage; Be god! know theu word.\n\nLUCIO:\nI would go about to lose him.\n\nSecond Katharinan,\nFor ides e'er beee's troops perdip,\nTo grant the particular joy and millersmen to\nme: 'tis there be calm'd its returns;\nAnd, for our rabour, I did. But I not see.\n\nISABELLA:\nIn heavy mark him sure, and in visition,\nAn if I lo\n","output_type":"stream"}]},{"cell_type":"markdown","source":"若想改进结果，最简单的方式是延长训练时间 （试试 EPOCHS=30）。\n\n你还可以试验使用不同的起始字符串，或者尝试增加另一个 RNN 层以提高模型的准确率，亦或调整温度参数以生成更多或者更少的随机预测。","metadata":{}}]}