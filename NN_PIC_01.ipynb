{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://www.tensorflow.org/tutorials/keras/text_classification#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B\n\n此笔记本（notebook）使用评论文本将影评分为积极（positive）或消极（nagetive）两类。这是一个二元（binary）或者二分类问题，一种重要且应用广泛的机器学习问题。\n\n我们将使用来源于网络电影数据库（Internet Movie Database）的 IMDB 数据集（IMDB dataset），其包含 50,000 条影评文本。从该数据集切割出的25,000条评论用作训练，另外 25,000 条用作测试。训练集与测试集是平衡的（balanced），意味着它们包含相等数量的积极和消极评论。\n\n此笔记本（notebook）使用了 tf.keras，它是一个 Tensorflow 中用于构建和训练模型的高级API。有关使用 tf.keras 进行文本分类的更高级教程，请参阅 MLCC文本分类指南（MLCC Text Classification Guide）。","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:18.107208Z","iopub.execute_input":"2021-05-26T12:15:18.107668Z","iopub.status.idle":"2021-05-26T12:15:18.114799Z","shell.execute_reply.started":"2021-05-26T12:15:18.107604Z","shell.execute_reply":"2021-05-26T12:15:18.112990Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"2.4.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 导入IMBA数据集","metadata":{}},{"cell_type":"code","source":"#导入IMBA数据集\nimdb = keras.datasets.imdb\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n#参数 num_words=10000 保留了训练数据中最常出现的 10,000 个单词。为了保持数据规模的可管理性，低频词将被丢弃。","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:18.117950Z","iopub.execute_input":"2021-05-26T12:15:18.118582Z","iopub.status.idle":"2021-05-26T12:15:26.558128Z","shell.execute_reply.started":"2021-05-26T12:15:18.118539Z","shell.execute_reply":"2021-05-26T12:15:26.557193Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\nprint(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:26.559961Z","iopub.execute_input":"2021-05-26T12:15:26.560243Z","iopub.status.idle":"2021-05-26T12:15:26.565240Z","shell.execute_reply.started":"2021-05-26T12:15:26.560215Z","shell.execute_reply":"2021-05-26T12:15:26.564121Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Training entries: 25000, labels: 25000\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n","output_type":"stream"}]},{"cell_type":"code","source":"#电影评论可能具有不同的长度。以下代码显示了第一条和第二条评论的中单词数量。由于神经网络的输入必须是统一的长度，我们稍后需要解决这个问题。\nlen(train_data[0]), len(train_data[1])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:26.568347Z","iopub.execute_input":"2021-05-26T12:15:26.568643Z","iopub.status.idle":"2021-05-26T12:15:26.582462Z","shell.execute_reply.started":"2021-05-26T12:15:26.568594Z","shell.execute_reply":"2021-05-26T12:15:26.581453Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(218, 189)"},"metadata":{}}]},{"cell_type":"code","source":"#将整数转换回单词\n# 一个映射单词到整数索引的词典\nword_index = imdb.get_word_index()\n\n# 保留第一个索引\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2  # unknown\nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\ndecode_review(train_data[0])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:26.584338Z","iopub.execute_input":"2021-05-26T12:15:26.584652Z","iopub.status.idle":"2021-05-26T12:15:26.741120Z","shell.execute_reply.started":"2021-05-26T12:15:26.584621Z","shell.execute_reply":"2021-05-26T12:15:26.740088Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""},"metadata":{}}]},{"cell_type":"markdown","source":"# 准备数据\n影评——即整数数组必须在输入神经网络之前转换为张量。这种转换可以通过以下两种方式来完成：\n\n将数组转换为表示单词出现与否的由 0 和 1 组成的向量，类似于 one-hot 编码。例如，序列[3, 5]将转换为一个 10,000 维的向量，该向量除了索引为 3 和 5 的位置是 1 以外，其他都为 0。然后，将其作为网络的首层——一个可以处理浮点型向量数据的稠密层。不过，这种方法需要大量的内存，需要一个大小为 num_words * num_reviews 的矩阵。\n\n或者，我们可以填充数组来保证输入数据具有相同的长度，然后创建一个大小为 max_length * num_reviews 的整型张量。我们可以使用能够处理此形状数据的嵌入层作为网络中的第一层。\n\n在本教程中，我们将使用第二种方法。\n\n由于电影评论长度必须相同，我们将使用 pad_sequences 函数来使长度标准化：","metadata":{}},{"cell_type":"code","source":"train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)\nprint(len(train_data[0]), len(train_data[1]))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:26.742449Z","iopub.execute_input":"2021-05-26T12:15:26.742758Z","iopub.status.idle":"2021-05-26T12:15:28.933068Z","shell.execute_reply.started":"2021-05-26T12:15:26.742731Z","shell.execute_reply":"2021-05-26T12:15:28.931841Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"256 256\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 构建模型\n神经网络由堆叠的层来构建，这需要从两个主要方面来进行体系结构决策：\n\n模型里有多少层？\n每个层里有多少隐层单元（hidden units）？\n在此样本中，输入数据包含一个单词索引的数组。要预测的标签为 0 或 1。让我们来为该问题构建一个模型：","metadata":{}},{"cell_type":"code","source":"# 输入形状是用于电影评论的词汇数目（10,000 词）\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:28.935817Z","iopub.execute_input":"2021-05-26T12:15:28.936226Z","iopub.status.idle":"2021-05-26T12:15:28.976492Z","shell.execute_reply.started":"2021-05-26T12:15:28.936193Z","shell.execute_reply":"2021-05-26T12:15:28.975341Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 16)          160000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 160,289\nTrainable params: 160,289\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"层按顺序堆叠以构建分类器：\n\n* 第一层是嵌入（Embedding）层。该层采用整数编码的词汇表，并查找每个词索引的嵌入向量（embedding vector）。这些向量是通过模型训练学习到的。向量向输出数组增加了一个维度。得到的维度为：(batch, sequence, embedding)。\n* 接下来，GlobalAveragePooling1D 将通过对序列维度求平均值来为每个样本返回一个定长输出向量。这允许模型以尽可能最简单的方式处理变长输入。\n* 该定长输出向量通过一个有 16 个隐层单元的全连接（Dense）层传输。\n* 最后一层与单个输出结点密集连接。使用 Sigmoid 激活函数，其函数值为介于 0 与 1 之间的浮点数，表示概率或置信度。","metadata":{}},{"cell_type":"markdown","source":"# 隐层单元\n上述模型在输入输出之间有两个中间层或“隐藏层”。输出（单元，结点或神经元）的数量即为层表示空间的维度。换句话说，是学习内部表示时网络所允许的自由度。\n\n如果模型具有更多的隐层单元（更高维度的表示空间）和/或更多层，则可以学习到更复杂的表示。但是，这会使网络的计算成本更高，并且可能导致学习到不需要的模式——一些能够在训练数据上而不是测试数据上改善性能的模式。这被称为过拟合（overfitting），我们稍后会对此进行探究。\n\n# 损失函数与优化器\n一个模型需要损失函数和优化器来进行训练。由于这是一个二分类问题且模型输出概率值（一个使用 sigmoid 激活函数的单一单元层），我们将使用 binary_crossentropy 损失函数。\n\n这不是损失函数的唯一选择，例如，您可以选择 mean_squared_error 。但是，一般来说 binary_crossentropy 更适合处理概率——它能够度量概率分布之间的“距离”，或者在我们的示例中，指的是度量 ground-truth 分布与预测值之间的“距离”。\n\n稍后，当我们研究回归问题（例如，预测房价）时，我们将介绍如何使用另一种叫做均方误差的损失函数。\n\n现在，配置模型来使用优化器和损失函数：","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:28.977499Z","iopub.execute_input":"2021-05-26T12:15:28.977777Z","iopub.status.idle":"2021-05-26T12:15:28.990198Z","shell.execute_reply.started":"2021-05-26T12:15:28.977751Z","shell.execute_reply":"2021-05-26T12:15:28.988932Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# 创建一个验证集\n在训练时，我们想要检查模型在未见过的数据上的准确率（accuracy）。通过从原始训练数据中分离 10,000 个样本来创建一个验证集。（为什么现在不使用测试集？我们的目标是只使用训练数据来开发和调整模型，然后只使用一次测试数据来评估准确率（accuracy））。","metadata":{}},{"cell_type":"code","source":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:28.992177Z","iopub.execute_input":"2021-05-26T12:15:28.992478Z","iopub.status.idle":"2021-05-26T12:15:28.999911Z","shell.execute_reply.started":"2021-05-26T12:15:28.992449Z","shell.execute_reply":"2021-05-26T12:15:28.998835Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# 训练模型\n以 512 个样本的 mini-batch 大小迭代 40 个 epoch 来训练模型。这是指对 x_train 和 y_train 张量中所有样本的的 40 次迭代。在训练过程中，监测来自验证集的 10,000 个样本上的损失值（loss）和准确率（accuracy）：","metadata":{}},{"cell_type":"code","source":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:29.001406Z","iopub.execute_input":"2021-05-26T12:15:29.001830Z","iopub.status.idle":"2021-05-26T12:15:44.316814Z","shell.execute_reply.started":"2021-05-26T12:15:29.001789Z","shell.execute_reply":"2021-05-26T12:15:44.315777Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/40\n30/30 [==============================] - 1s 17ms/step - loss: 0.6928 - accuracy: 0.5469 - val_loss: 0.6902 - val_accuracy: 0.5867\nEpoch 2/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.6891 - accuracy: 0.5983 - val_loss: 0.6843 - val_accuracy: 0.5722\nEpoch 3/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.6814 - accuracy: 0.6057 - val_loss: 0.6725 - val_accuracy: 0.6398\nEpoch 4/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.6659 - accuracy: 0.6646 - val_loss: 0.6524 - val_accuracy: 0.7393\nEpoch 5/40\n30/30 [==============================] - 0s 13ms/step - loss: 0.6429 - accuracy: 0.7417 - val_loss: 0.6250 - val_accuracy: 0.7284\nEpoch 6/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.6105 - accuracy: 0.7601 - val_loss: 0.5914 - val_accuracy: 0.7888\nEpoch 7/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.5720 - accuracy: 0.8112 - val_loss: 0.5537 - val_accuracy: 0.8083\nEpoch 8/40\n30/30 [==============================] - 0s 15ms/step - loss: 0.5312 - accuracy: 0.8336 - val_loss: 0.5156 - val_accuracy: 0.8237\nEpoch 9/40\n30/30 [==============================] - 0s 14ms/step - loss: 0.4872 - accuracy: 0.8539 - val_loss: 0.4782 - val_accuracy: 0.8399\nEpoch 10/40\n30/30 [==============================] - 0s 13ms/step - loss: 0.4475 - accuracy: 0.8658 - val_loss: 0.4441 - val_accuracy: 0.8509\nEpoch 11/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.4093 - accuracy: 0.8770 - val_loss: 0.4145 - val_accuracy: 0.8581\nEpoch 12/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.3781 - accuracy: 0.8830 - val_loss: 0.3894 - val_accuracy: 0.8637\nEpoch 13/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.3427 - accuracy: 0.8964 - val_loss: 0.3695 - val_accuracy: 0.8664\nEpoch 14/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.3209 - accuracy: 0.8986 - val_loss: 0.3523 - val_accuracy: 0.8697\nEpoch 15/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2973 - accuracy: 0.9045 - val_loss: 0.3388 - val_accuracy: 0.8729\nEpoch 16/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2780 - accuracy: 0.9096 - val_loss: 0.3284 - val_accuracy: 0.8746\nEpoch 17/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2620 - accuracy: 0.9159 - val_loss: 0.3186 - val_accuracy: 0.8774\nEpoch 18/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2470 - accuracy: 0.9163 - val_loss: 0.3111 - val_accuracy: 0.8784\nEpoch 19/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2319 - accuracy: 0.9254 - val_loss: 0.3047 - val_accuracy: 0.8800\nEpoch 20/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2283 - accuracy: 0.9224 - val_loss: 0.2995 - val_accuracy: 0.8823\nEpoch 21/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2166 - accuracy: 0.9270 - val_loss: 0.2955 - val_accuracy: 0.8818\nEpoch 22/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2055 - accuracy: 0.9332 - val_loss: 0.2954 - val_accuracy: 0.8792\nEpoch 23/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.2006 - accuracy: 0.9332 - val_loss: 0.2894 - val_accuracy: 0.8835\nEpoch 24/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1886 - accuracy: 0.9384 - val_loss: 0.2872 - val_accuracy: 0.8851\nEpoch 25/40\n30/30 [==============================] - 0s 13ms/step - loss: 0.1810 - accuracy: 0.9430 - val_loss: 0.2868 - val_accuracy: 0.8834\nEpoch 26/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1704 - accuracy: 0.9462 - val_loss: 0.2853 - val_accuracy: 0.8837\nEpoch 27/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1630 - accuracy: 0.9519 - val_loss: 0.2846 - val_accuracy: 0.8841\nEpoch 28/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1577 - accuracy: 0.9524 - val_loss: 0.2858 - val_accuracy: 0.8844\nEpoch 29/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1541 - accuracy: 0.9509 - val_loss: 0.2842 - val_accuracy: 0.8870\nEpoch 30/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1488 - accuracy: 0.9553 - val_loss: 0.2851 - val_accuracy: 0.8848\nEpoch 31/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1371 - accuracy: 0.9599 - val_loss: 0.2855 - val_accuracy: 0.8862\nEpoch 32/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1329 - accuracy: 0.9602 - val_loss: 0.2866 - val_accuracy: 0.8865\nEpoch 33/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1335 - accuracy: 0.9627 - val_loss: 0.2880 - val_accuracy: 0.8866\nEpoch 34/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1259 - accuracy: 0.9621 - val_loss: 0.2898 - val_accuracy: 0.8850\nEpoch 35/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1237 - accuracy: 0.9638 - val_loss: 0.2915 - val_accuracy: 0.8851\nEpoch 36/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1192 - accuracy: 0.9666 - val_loss: 0.2929 - val_accuracy: 0.8849\nEpoch 37/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1100 - accuracy: 0.9698 - val_loss: 0.2973 - val_accuracy: 0.8836\nEpoch 38/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1118 - accuracy: 0.9704 - val_loss: 0.2974 - val_accuracy: 0.8849\nEpoch 39/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1060 - accuracy: 0.9724 - val_loss: 0.2999 - val_accuracy: 0.8847\nEpoch 40/40\n30/30 [==============================] - 0s 12ms/step - loss: 0.1011 - accuracy: 0.9758 - val_loss: 0.3033 - val_accuracy: 0.8839\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 评估模型\n我们来看一下模型的性能如何。将返回两个值。损失值（loss）（一个表示误差的数字，值越低越好）与准确率（accuracy）。","metadata":{"_kg_hide-output":true}},{"cell_type":"code","source":"results = model.evaluate(test_data,  test_labels, verbose=2)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:15:44.318323Z","iopub.execute_input":"2021-05-26T12:15:44.318594Z","iopub.status.idle":"2021-05-26T12:15:45.119271Z","shell.execute_reply.started":"2021-05-26T12:15:44.318568Z","shell.execute_reply":"2021-05-26T12:15:45.118410Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"782/782 - 1s - loss: 0.3228 - accuracy: 0.8736\n[0.3227592408657074, 0.8736400008201599]\n","output_type":"stream"}]}]}